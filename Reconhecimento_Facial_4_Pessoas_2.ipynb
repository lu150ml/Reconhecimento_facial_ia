{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9e3a5263",
      "metadata": {
        "id": "9e3a5263"
      },
      "source": [
        "\n",
        "# Reconhecimento Facial — 4 Pessoas (Você + 3 colegas)\n",
        "\n",
        "Este notebook cria um pipeline completo de **reconhecimento facial** para 4 pessoas usando:\n",
        "\n",
        "- **Detecção e alinhamento:** `MTCNN` (do pacote `facenet-pytorch`)\n",
        "- **Embeddings faciais:** `InceptionResnetV1` (FaceNet pré‑treinado em VGGFace2, via `facenet-pytorch`)\n",
        "- **Classificador:** `SVM` (RBF) do `scikit-learn` sobre os embeddings\n",
        "\n",
        "Por que assim?\n",
        "- Para datasets pequenos, **extrair embeddings pré‑treinados** e treinar um **classificador leve** costuma dar\n",
        "  resultados muito melhores e treina bem mais rápido do que treinar uma CNN do zero.\n",
        "- Se preferir, há também uma **Opção B (Transfer Learning com Keras/MobileNetV2)** no final.\n",
        "\n",
        "> **Como organizar seus dados** (recomendado):\n",
        ">\n",
        "> ```text\n",
        "> dados_brutos/\n",
        "> ├── pessoa1_matheus/\n",
        "> │   ├── img001.jpg\n",
        "> │   ├── img002.jpg\n",
        "> │   └── ...\n",
        "> ├── pessoa2_colegaA/\n",
        "> ├── pessoa3_colegaB/\n",
        "> └── pessoa4_colegaC/\n",
        "> ```\n",
        ">\n",
        "> Ideal ter **50+ fotos por pessoa** cobrindo variações de luz, pose e expressão.\n",
        "> O notebook vai **detectar/alinha** as faces e salvar em `dados_processados/`.\n",
        ">\n",
        "> **Observação:** Este notebook foi gerado tomando como base o seu arquivo `\"[ Aula 17 ] - CKP04.ipynb\"`.\n",
        "> Você pode copiar/colar células deste novo notebook para o seu, ou rodar este diretamente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a570e3f8",
      "metadata": {
        "id": "a570e3f8"
      },
      "source": [
        "\n",
        "## 1) Instalação de dependências (rode apenas se precisar)\n",
        "\n",
        "Se der erro de import, rode este bloco. Caso já tenha tudo instalado, pode pular.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad80a40d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ad80a40d",
        "outputId": "891c0af7-ac24-492c-a6b6-d7b4ddf87624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.2)\n",
            "Requirement already satisfied: facenet-pytorch==2.5.3 in /usr/local/lib/python3.12/dist-packages (2.5.3)\n",
            "Collecting mtcnn==0.1.1\n",
            "  Using cached mtcnn-0.1.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting opencv-python==4.10.0.84\n",
            "  Using cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting scikit-learn==1.5.1\n",
            "  Using cached scikit_learn-1.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting joblib==1.4.2\n",
            "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch==2.5.3) (2.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch==2.5.3) (2.32.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch==2.5.3) (0.23.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch==2.5.3) (11.3.0)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mtcnn==0.1.1) (3.10.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.1) (1.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.1) (3.6.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras>=2.0.0->mtcnn==0.1.1) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=2.0.0->mtcnn==0.1.1) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=2.0.0->mtcnn==0.1.1) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=2.0.0->mtcnn==0.1.1) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=2.0.0->mtcnn==0.1.1) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=2.0.0->mtcnn==0.1.1) (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras>=2.0.0->mtcnn==0.1.1) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras>=2.0.0->mtcnn==0.1.1) (4.14.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->facenet-pytorch==2.5.3) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->facenet-pytorch==2.5.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->facenet-pytorch==2.5.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->facenet-pytorch==2.5.3) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=2.0.0->mtcnn==0.1.1) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=2.0.0->mtcnn==0.1.1) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.0.0->mtcnn==0.1.1) (0.1.2)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->facenet-pytorch==2.5.3) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision->facenet-pytorch==2.5.3) (3.0.2)\n",
            "Using cached mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "Using cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
            "Using cached scikit_learn-1.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Installing collected packages: opencv-python, joblib, scikit-learn, mtcnn\n",
            "\u001b[2K  Attempting uninstall: opencv-python\n",
            "\u001b[2K    Found existing installation: opencv-python 4.12.0.88\n",
            "\u001b[2K    Uninstalling opencv-python-4.12.0.88:\n",
            "\u001b[2K      Successfully uninstalled opencv-python-4.12.0.88\n",
            "\u001b[2K  Attempting uninstall: joblib\n",
            "\u001b[2K    Found existing installation: joblib 1.5.1\n",
            "\u001b[2K    Uninstalling joblib-1.5.1:\n",
            "\u001b[2K      Successfully uninstalled joblib-1.5.1\n",
            "\u001b[2K  Attempting uninstall: scikit-learn\n",
            "\u001b[2K    Found existing installation: scikit-learn 1.7.1\n",
            "\u001b[2K    Uninstalling scikit-learn-1.7.1:\n",
            "\u001b[2K      Successfully uninstalled scikit-learn-1.7.1\n",
            "\u001b[2K  Attempting uninstall: mtcnn\n",
            "\u001b[2K    Found existing installation: mtcnn 1.0.0\n",
            "\u001b[2K    Uninstalling mtcnn-1.0.0:\n",
            "\u001b[2K      Successfully uninstalled mtcnn-1.0.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [mtcnn]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.4.2 mtcnn-0.1.1 opencv-python-4.10.0.84 scikit-learn-1.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2",
                  "joblib",
                  "sklearn"
                ]
              },
              "id": "58b0217f569249aebff7af9a7199686d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib==3.9.0\n",
            "  Using cached matplotlib-3.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pandas==2.2.2\n",
            "  Downloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.0) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib==3.9.0) (1.17.0)\n",
            "Using cached matplotlib-3.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "Downloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Installing collected packages: numpy, pandas, matplotlib\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 2.2.6\n",
            "\u001b[2K    Uninstalling numpy-2.2.6:\n",
            "\u001b[2K      Successfully uninstalled numpy-2.2.6\n",
            "\u001b[2K  Attempting uninstall: pandas\n",
            "\u001b[2K    Found existing installation: pandas 2.3.1\n",
            "\u001b[2K    Uninstalling pandas-2.3.1:\n",
            "\u001b[2K      Successfully uninstalled pandas-2.3.1\n",
            "\u001b[2K  Attempting uninstall: matplotlib\n",
            "\u001b[2K    Found existing installation: matplotlib 3.10.5\n",
            "\u001b[2K    Uninstalling matplotlib-3.10.5:\n",
            "\u001b[2K      Successfully uninstalled matplotlib-3.10.5\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [matplotlib]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.20.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.9.0 numpy-1.26.4 pandas-2.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pandas"
                ]
              },
              "id": "b95958b3c9674bc7a2694bda5adb8ab0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.15.0 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0, 2.19.1, 2.20.0rc0, 2.20.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.15.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "#Se precisar, descomente as linhas abaixo\n",
        "!pip install --upgrade pip\n",
        "!pip install facenet-pytorch==2.5.3 mtcnn==0.1.1 opencv-python==4.10.0.84 scikit-learn==1.5.1 joblib==1.4.2\n",
        "!pip install matplotlib==3.9.0 pandas==2.2.2 numpy==1.26.4\n",
        "#Para a Opção B (Keras/MobileNetV2):\n",
        "!pip install tensorflow==2.15.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20bab446",
      "metadata": {
        "id": "20bab446"
      },
      "source": [
        "\n",
        "## 2) Imports e configuração\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfa99e72-e7c1-4f96-8b16-cf102cc50946",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfa99e72-e7c1-4f96-8b16-cf102cc50946",
        "outputId": "cb3c21c5-fac4-48ce-e898-c5537c00c0b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.9.0)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.10.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.4.2)\n",
            "Collecting joblib\n",
            "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.10.0.84)\n",
            "Collecting opencv-python\n",
            "  Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.12/dist-packages (2.5.3)\n",
            "Collecting facenet-pytorch\n",
            "  Using cached facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mtcnn in /usr/local/lib/python3.12/dist-packages (0.1.1)\n",
            "Collecting mtcnn\n",
            "  Using cached mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: pillow-heif in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "INFO: pip is looking at multiple versions of facenet-pytorch to determine which version is compatible with other requirements. This could take a while.\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (2.32.4)\n",
            "Requirement already satisfied: lz4>=4.3.3 in /usr/local/lib/python3.12/dist-packages (from mtcnn) (4.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.20.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.20.0)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->facenet-pytorch) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->facenet-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->facenet-pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->facenet-pytorch) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Using cached pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "Using cached matplotlib-3.10.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "Using cached scikit_learn-1.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
            "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
            "Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
            "Using cached numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
            "Using cached mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "Installing collected packages: numpy, joblib, pandas, opencv-python, mtcnn, scikit-learn, matplotlib\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 1.26.4\n",
            "\u001b[2K    Uninstalling numpy-1.26.4:\n",
            "\u001b[2K      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[2K  Attempting uninstall: joblib\n",
            "\u001b[2K    Found existing installation: joblib 1.4.2\n",
            "\u001b[2K    Uninstalling joblib-1.4.2:\n",
            "\u001b[2K      Successfully uninstalled joblib-1.4.2\n",
            "\u001b[2K  Attempting uninstall: pandas\n",
            "\u001b[2K    Found existing installation: pandas 2.2.2\n",
            "\u001b[2K    Uninstalling pandas-2.2.2:\n",
            "\u001b[2K      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[2K  Attempting uninstall: opencv-python\n",
            "\u001b[2K    Found existing installation: opencv-python 4.10.0.84\n",
            "\u001b[2K    Uninstalling opencv-python-4.10.0.84:\n",
            "\u001b[2K      Successfully uninstalled opencv-python-4.10.0.84\n",
            "\u001b[2K  Attempting uninstall: mtcnn\n",
            "\u001b[2K    Found existing installation: mtcnn 0.1.1\n",
            "\u001b[2K    Uninstalling mtcnn-0.1.1:\n",
            "\u001b[2K      Successfully uninstalled mtcnn-0.1.1\n",
            "\u001b[2K  Attempting uninstall: scikit-learn\n",
            "\u001b[2K    Found existing installation: scikit-learn 1.5.1\n",
            "\u001b[2K    Uninstalling scikit-learn-1.5.1:\n",
            "\u001b[2K      Successfully uninstalled scikit-learn-1.5.1\n",
            "\u001b[2K  Attempting uninstall: matplotlib\n",
            "\u001b[2K    Found existing installation: matplotlib 3.9.0\n",
            "\u001b[2K    Uninstalling matplotlib-3.9.0:\n",
            "\u001b[2K      Successfully uninstalled matplotlib-3.9.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [matplotlib]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.20.0 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.5.1 matplotlib-3.10.5 mtcnn-1.0.0 numpy-2.2.6 opencv-python-4.12.0.88 pandas-2.3.1 scikit-learn-1.7.1\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install --upgrade \\\n",
        "    numpy \\\n",
        "    pandas \\\n",
        "    matplotlib \\\n",
        "    scikit-learn \\\n",
        "    joblib \\\n",
        "    opencv-python \\\n",
        "    facenet-pytorch \\\n",
        "    mtcnn \\\n",
        "    pillow \\\n",
        "    pillow-heif \\\n",
        "    torch torchvision torchaudio \\\n",
        "    tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "397d49bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "397d49bb",
        "outputId": "55e58f73-a919-4306-a6e3-94821f1cfdb3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Torch / facenet-pytorch\n",
        "import torch\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Reprodutibilidade\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Se tiver GPU disponível, ótimo!\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b8d36cd",
      "metadata": {
        "id": "6b8d36cd"
      },
      "source": [
        "\n",
        "## 3) Defina os caminhos das pastas\n",
        "\n",
        "- `dados_brutos/`: suas fotos originais (múltiplas por pessoa, em subpastas).\n",
        "- `dados_processados/`: faces detectadas/alinhadas que o notebook vai gerar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9CeToUF10rY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "b9CeToUF10rY",
        "outputId": "4394a657-b1fd-4e6e-ccbc-8ffce04bb952"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dados_brutos'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4030000891.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontagem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mcontagem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontar_imagens_por_pessoa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mtotal_arquivos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontagem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4030000891.py\u001b[0m in \u001b[0;36mcontar_imagens_por_pessoa\u001b[0;34m(pasta_root)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcontar_imagens_por_pessoa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpasta_root\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mcontagem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0msubpastas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpasta_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpessoa_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubpastas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0mspecial\u001b[0m \u001b[0mentries\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'..'\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mincluded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \"\"\"\n\u001b[0;32m-> 1056\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_child_relpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dados_brutos'"
          ]
        }
      ],
      "source": [
        "# Verificação das 4 pastas e contagem de imagens por pessoa em dados_brutos/\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import shutil\n",
        "\n",
        "VALID_EXT = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".heic\"}\n",
        "\n",
        "root = Path(\"dados_brutos\")\n",
        "\n",
        "# Corrige casos de zip aninhado: dados_brutos/dados_brutos/...\n",
        "if (root / \"dados_brutos\").exists():\n",
        "    root = root / \"dados_brutos\"\n",
        "\n",
        "# Remove pasta extra do macOS, se existir\n",
        "if (root / \"__MACOSX\").exists():\n",
        "    shutil.rmtree(root / \"__MACOSX\", ignore_errors=True)\n",
        "\n",
        "def contar_imagens_por_pessoa(pasta_root: Path):\n",
        "    contagem = {}\n",
        "    subpastas = [p for p in pasta_root.iterdir() if p.is_dir() and not p.name.startswith(\".\")]\n",
        "    for pessoa_dir in sorted(subpastas):\n",
        "        n = 0\n",
        "        for f in pessoa_dir.rglob(\"*\"):\n",
        "            if f.is_file() and f.suffix.lower() in VALID_EXT:\n",
        "                n += 1\n",
        "        contagem[pessoa_dir.name] = n\n",
        "    return contagem\n",
        "\n",
        "contagem = contar_imagens_por_pessoa(root)\n",
        "\n",
        "total_arquivos = sum(contagem.values())\n",
        "classes = sorted(contagem.keys())\n",
        "\n",
        "print(f\"Raiz analisada: {root.resolve()}\")\n",
        "print(f\"Total de classes encontradas: {len(classes)} -> {classes}\")\n",
        "print(f\"Total de arquivos válidos: {total_arquivos}\\n\")\n",
        "\n",
        "print(\"Contagem por classe:\")\n",
        "for k in classes:\n",
        "    print(f\"- {k}: {contagem[k]} imagens\")\n",
        "\n",
        "faltando = [k for k,v in contagem.items() if v == 0]\n",
        "if faltando:\n",
        "    print(\"\\nAtenção: as classes abaixo estão vazias (0 imagens válidas):\")\n",
        "    for k in faltando:\n",
        "        print(f\"- {k}\")\n",
        "\n",
        "# Se quiser exigir exatamente 4 classes, descomente:\n",
        "# if len(classes) != 4:\n",
        "#     raise ValueError(f\"Esperadas 4 classes, mas encontrei {len(classes)}. Verifique nomes/pastas.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "229b18e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "229b18e5",
        "outputId": "f5f1de2a-8a21-4279-ade6-969abfd59f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brutos: /content/dados_brutos\n",
            "Processados: /content/dados_processados\n"
          ]
        }
      ],
      "source": [
        "\n",
        "BASE = Path().resolve()\n",
        "\n",
        "# Ajuste estes nomes se quiser\n",
        "PASTA_BRUTOS = BASE / \"dados_brutos\"\n",
        "PASTA_PROC   = BASE / \"dados_processados\"\n",
        "\n",
        "PASTA_PROC.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Brutos:\", PASTA_BRUTOS)\n",
        "print(\"Processados:\", PASTA_PROC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a9f303",
      "metadata": {
        "id": "a5a9f303"
      },
      "source": [
        "\n",
        "## 4) Detecção e alinhamento das faces (MTCNN)\n",
        "\n",
        "Este passo percorre `dados_brutos/`, detecta/alinha a face principal e salva faces 160x160 em `dados_processados/`,\n",
        "mantendo a **mesma estrutura de pastas por pessoa**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eaa9b5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "8eaa9b5c",
        "outputId": "bec05748-dcb1-46f6-858e-586ba2260450"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/dados_brutos'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2210897582.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Faces processadas: {total_ok}/{total_in} imagens\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mprocessar_pasta_de_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPASTA_BRUTOS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPASTA_PROC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2210897582.py\u001b[0m in \u001b[0;36mprocessar_pasta_de_faces\u001b[0;34m(pasta_origem, pasta_destino)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtotal_ok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpessoa_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpasta_origem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpessoa_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0mspecial\u001b[0m \u001b[0mentries\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'..'\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mincluded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \"\"\"\n\u001b[0;32m-> 1056\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_child_relpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dados_brutos'"
          ]
        }
      ],
      "source": [
        "\n",
        "# Detector (ajuste thresholds se estiver perdendo/pegando faces demais)\n",
        "mtcnn = MTCNN(image_size=160, margin=20, post_process=True, device=device)\n",
        "\n",
        "def processar_pasta_de_faces(pasta_origem, pasta_destino):\n",
        "    pasta_origem = Path(pasta_origem)\n",
        "    pasta_destino = Path(pasta_destino)\n",
        "    pasta_destino.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    total_in = 0\n",
        "    total_ok = 0\n",
        "\n",
        "    for pessoa_dir in pasta_origem.iterdir():\n",
        "        if not pessoa_dir.is_dir():\n",
        "            continue\n",
        "        nome = pessoa_dir.name\n",
        "        destino_pessoa = pasta_destino / nome\n",
        "        destino_pessoa.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        for img_path in pessoa_dir.glob(\"*.*\"):\n",
        "            if img_path.suffix.lower() not in {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\"}:\n",
        "                continue\n",
        "            total_in += 1\n",
        "            # Leitura com OpenCV (BGR)\n",
        "            img_bgr = cv2.imread(str(img_path))\n",
        "            if img_bgr is None:\n",
        "                continue\n",
        "            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "            # Detecta/alinha e retorna PIL Image 160x160\n",
        "            face = mtcnn(img_rgb, save_path=None)\n",
        "            if face is None:\n",
        "                continue\n",
        "            # Converte para numpy e salva\n",
        "            face_np = face.permute(1,2,0).cpu().numpy()  # (160,160,3) RGB float[0..1]\n",
        "            face_bgr = cv2.cvtColor((face_np*255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
        "            out_path = destino_pessoa / img_path.name\n",
        "            cv2.imwrite(str(out_path), face_bgr)\n",
        "            total_ok += 1\n",
        "\n",
        "    print(f\"Faces processadas: {total_ok}/{total_in} imagens\")\n",
        "\n",
        "processar_pasta_de_faces(PASTA_BRUTOS, PASTA_PROC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1cc0fdf",
      "metadata": {
        "id": "a1cc0fdf"
      },
      "source": [
        "\n",
        "## 5) Carregar caminhos e fazer split (train/val/test)\n",
        "\n",
        "Vamos listar as imagens processadas e separar em treino/validação/teste de forma estratificada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d947a094",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "d947a094",
        "outputId": "868a613c-4a0b-47fc-e371-e9e7795b44c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total imagens processadas: 0\n",
            "Classes encontradas: []\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Certifique-se de ter exatamente 4 pastas (4 pessoas)!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3530726977.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total imagens processadas:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classes encontradas:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Certifique-se de ter exatamente 4 pastas (4 pessoas)!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Split train+val/test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Certifique-se de ter exatamente 4 pastas (4 pessoas)!"
          ]
        }
      ],
      "source": [
        "\n",
        "def listar_imagens_com_rotulos(pasta):\n",
        "    paths = []\n",
        "    labels = []\n",
        "    for pessoa_dir in Path(pasta).iterdir():\n",
        "        if not pessoa_dir.is_dir():\n",
        "            continue\n",
        "        label = pessoa_dir.name\n",
        "        for img in pessoa_dir.glob(\"*.*\"):\n",
        "            if img.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\"}:\n",
        "                paths.append(str(img))\n",
        "                labels.append(label)\n",
        "    return np.array(paths), np.array(labels)\n",
        "\n",
        "X_paths, y_labels = listar_imagens_com_rotulos(PASTA_PROC)\n",
        "print(\"Total imagens processadas:\", len(X_paths))\n",
        "print(\"Classes encontradas:\", sorted(set(y_labels)))\n",
        "assert len(set(y_labels)) == 4, \"Certifique-se de ter exatamente 4 pastas (4 pessoas)!\"\n",
        "\n",
        "# Split train+val/test\n",
        "X_tmp, X_test, y_tmp, y_test = train_test_split(\n",
        "    X_paths, y_labels, test_size=0.2, random_state=SEED, stratify=y_labels\n",
        ")\n",
        "# Split train/val\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_tmp, y_tmp, test_size=0.2, random_state=SEED, stratify=y_tmp\n",
        ")\n",
        "\n",
        "len(X_train), len(X_val), len(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed82b22e",
      "metadata": {
        "id": "ed82b22e"
      },
      "source": [
        "\n",
        "## 6) Extrair embeddings (FaceNet)\n",
        "\n",
        "Carregamos o `InceptionResnetV1` pré‑treinado (VGGFace2) e extraímos um vetor de **512 dimensões** por face.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a382513f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "31642abc58cf4c0fb9d7ffac2b3412b3",
            "037a7613405f4611a07c789e21ee0906",
            "2a965a95eb66414998c013e73ffbf096",
            "2246ede09d46419882f8394e989b8358",
            "c5feb14aa3744c2fb4e8a80de4893632",
            "67bd5af61761458c8d5588cee012b94c",
            "be59e2bb1082484db8b996c223eab63f",
            "0d8bbf8a14b6497596070b66a1ed7c21",
            "781dee9b8fb649aeb784db129d81122a",
            "b2a499975c9048e69ffffc445cf27265",
            "9805257aa0294f3b93d3994d741cd443"
          ]
        },
        "id": "a382513f",
        "outputId": "f089406f-e6eb-46a2-b8c4-c2d14f397e08"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31642abc58cf4c0fb9d7ffac2b3412b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2289113620.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Gera embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mE_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths_to_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mE_val\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mpaths_to_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mE_test\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpaths_to_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# Carrega FaceNet (pode baixar pesos na 1ª vez)\n",
        "embedder = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "def carregar_img_rgb(path):\n",
        "    bgr = cv2.imread(path)\n",
        "    if bgr is None:\n",
        "        return None\n",
        "    return cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def img_to_embedding(img_rgb):\n",
        "    # img_rgb: np.array (160,160,3) RGB uint8\n",
        "    # Normaliza para [0,1] e para tensor (1,3,160,160)\n",
        "    img_norm = (img_rgb.astype(np.float32) / 255.0)\n",
        "    tensor = torch.from_numpy(img_norm).permute(2,0,1).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        emb = embedder(tensor).cpu().numpy()[0]  # (512,)\n",
        "    return emb\n",
        "\n",
        "def paths_to_embeddings(paths):\n",
        "    E = []\n",
        "    for p in paths:\n",
        "        img = carregar_img_rgb(p)\n",
        "        if img is None:\n",
        "            # falha rara de leitura — cria embedding zero (ou pule)\n",
        "            E.append(np.zeros(512, dtype=np.float32))\n",
        "            continue\n",
        "        emb = img_to_embedding(img)\n",
        "        E.append(emb.astype(np.float32))\n",
        "    return np.stack(E)\n",
        "\n",
        "# Gera embeddings\n",
        "E_train = paths_to_embeddings(X_train)\n",
        "E_val   = paths_to_embeddings(X_val)\n",
        "E_test  = paths_to_embeddings(X_test)\n",
        "\n",
        "E_train.shape, E_val.shape, E_test.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jz4YnGnqsMZQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz4YnGnqsMZQ",
        "outputId": "aa19cf6f-3a9a-426d-f04f-481922256c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dados_processados/Luis-rm559100/df193e2c-ab94-4146-b7bd-f51058fe113f.JPG\n"
          ]
        }
      ],
      "source": [
        "print('/content/dados_processados/Luis-rm559100/df193e2c-ab94-4146-b7bd-f51058fe113f.JPG')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e709a18a",
      "metadata": {
        "id": "e709a18a"
      },
      "source": [
        "\n",
        "## 7) Treinar classificador (SVM RBF)\n",
        "\n",
        "Usamos `SVC(C=10, kernel='rbf', probability=True)`. Sinta‑se livre para ajustar `C`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f9c4e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "43f9c4e8",
        "outputId": "c92e1e19-ce2f-4ebd-aabd-2516f88d8a21"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2193640237.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_train_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_val_enc\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_test_enc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_val_enc   = le.transform(y_val)\n",
        "y_test_enc  = le.transform(y_test)\n",
        "\n",
        "svm = SVC(C=10, kernel='rbf', gamma='scale', probability=True, random_state=SEED)\n",
        "svm.fit(E_train, y_train_enc)\n",
        "\n",
        "print(\"Acurácia (val):\", svm.score(E_val, y_val_enc))\n",
        "print(\"Acurácia (test):\", svm.score(E_test, y_test_enc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "108ab591",
      "metadata": {
        "id": "108ab591"
      },
      "source": [
        "\n",
        "## 8) Métricas e Matriz de Confusão\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0e202b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "d0e202b7",
        "outputId": "a8cd5f5d-1657-4285-e545-42f8055be7d0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'svm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1297628121.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'svm' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "y_pred = svm.predict(E_test)\n",
        "print(classification_report(y_test_enc, y_pred, target_names=le.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_test_enc, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
        "disp.plot(xticks_rotation=45)\n",
        "plt.title(\"Matriz de Confusão — Teste\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "163960a5",
      "metadata": {
        "id": "163960a5"
      },
      "source": [
        "\n",
        "## 9) Salvar modelo e rótulos\n",
        "\n",
        "Isto permite carregar rapidamente o classificador depois, sem precisar re-treinar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79df88d1",
      "metadata": {
        "id": "79df88d1"
      },
      "outputs": [],
      "source": [
        "\n",
        "Path(\"modelos\").mkdir(exist_ok=True)\n",
        "joblib.dump(svm, \"modelos/svm_faces.pkl\")\n",
        "joblib.dump(le,  \"modelos/label_encoder.pkl\")\n",
        "print(\"Arquivos salvos em ./modelos/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16965e0f",
      "metadata": {
        "id": "16965e0f"
      },
      "source": [
        "\n",
        "## 10) Inferência em uma imagem (exemplo)\n",
        "\n",
        "Passe o caminho de uma imagem com rosto (de preferência **criada pelo passo de processamento**).\n",
        "Se quiser passar uma imagem **bruta**, use a função `preprocessar_face_bruta` abaixo para alinhar antes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46d7cfd4",
      "metadata": {
        "id": "46d7cfd4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def prever_arquivo_imagem(path_img, threshold=0.60):\n",
        "    # Carrega modelos\n",
        "    clf = joblib.load(\"modelos/svm_faces.pkl\")\n",
        "    enc = joblib.load(\"modelos/label_encoder.pkl\")\n",
        "\n",
        "    img = carregar_img_rgb(path_img)\n",
        "    if img is None:\n",
        "        raise ValueError(\"Imagem não encontrada ou inválida.\")\n",
        "    emb = img_to_embedding(img)\n",
        "    proba = clf.predict_proba([emb])[0]\n",
        "    idx = int(proba.argmax())\n",
        "    nome = enc.inverse_transform([idx])[0]\n",
        "    conf = float(proba[idx])\n",
        "\n",
        "    # threshold simples (opcional) para rejeitar \"desconhecido\"\n",
        "    if conf < threshold:\n",
        "        nome = \"desconhecido\"\n",
        "    return nome, conf\n",
        "\n",
        "# Exemplo de uso (ajuste o caminho):\n",
        "# nome, conf = prever_arquivo_imagem(\"dados_processados/pessoa1_matheus/img001.jpg\")\n",
        "# print(nome, conf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f61f80f8",
      "metadata": {
        "id": "f61f80f8"
      },
      "source": [
        "\n",
        "### (Opcional) Preprocessar rosto diretamente de uma imagem **bruta**\n",
        "\n",
        "Se quiser fazer inferência em imagens que **não** passaram pelo passo de detecção/alinhamento, use esta função.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc434f53",
      "metadata": {
        "id": "cc434f53"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocessar_face_bruta(path_img_bruto):\n",
        "    img_bgr = cv2.imread(str(path_img_bruto))\n",
        "    if img_bgr is None:\n",
        "        return None\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    face = mtcnn(img_rgb, save_path=None)\n",
        "    if face is None:\n",
        "        return None\n",
        "    face_np = face.permute(1,2,0).cpu().numpy()  # (160,160,3) float [0..1]\n",
        "    face_bgr = cv2.cvtColor((face_np*255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
        "    return face_bgr\n",
        "\n",
        "def prever_imagem_bruta(path_img_bruto, threshold=0.60):\n",
        "    face_bgr = preprocessar_face_bruta(path_img_bruto)\n",
        "    if face_bgr is None:\n",
        "        return \"sem_face\", 0.0\n",
        "    # converter BGR->RGB e embutir\n",
        "    img_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
        "    emb = img_to_embedding(img_rgb)\n",
        "    clf = joblib.load(\"modelos/svm_faces.pkl\")\n",
        "    enc = joblib.load(\"modelos/label_encoder.pkl\")\n",
        "    proba = clf.predict_proba([emb])[0]\n",
        "    idx = int(proba.argmax())\n",
        "    nome = enc.inverse_transform([idx])[0]\n",
        "    conf = float(proba[idx])\n",
        "    if conf < threshold:\n",
        "        nome = \"desconhecido\"\n",
        "    return nome, conf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaGv4VqivTj7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaGv4VqivTj7",
        "outputId": "5e5f9e8b-0970-46f1-f674-d2231193431f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('sem_face', 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "prever_imagem_bruta('/content/IMG_3250.jpeg',threshold=0.60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8013a7a",
      "metadata": {
        "id": "a8013a7a"
      },
      "source": [
        "\n",
        "## 11) (Opcional) Webcam em tempo real\n",
        "\n",
        "Detecta rosto com MTCNN, tira embedding e classifica com o SVM. **Pressione `q` para sair.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dabe936",
      "metadata": {
        "id": "0dabe936"
      },
      "outputs": [],
      "source": [
        "\n",
        "def webcam_demo(threshold=0.60):\n",
        "    clf = joblib.load(\"modelos/svm_faces.pkl\")\n",
        "    enc = joblib.load(\"modelos/label_encoder.pkl\")\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Não consegui abrir a webcam.\")\n",
        "        return\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            # OpenCV vem em BGR\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            face = mtcnn(rgb)\n",
        "            if face is not None:\n",
        "                face_np = face.permute(1,2,0).cpu().numpy()\n",
        "                face_rgb = (face_np*255).astype(np.uint8)\n",
        "                emb = img_to_embedding(face_rgb)\n",
        "                proba = clf.predict_proba([emb])[0]\n",
        "                idx = int(proba.argmax())\n",
        "                nome = enc.inverse_transform([idx])[0]\n",
        "                conf = float(proba[idx])\n",
        "                if conf < threshold:\n",
        "                    nome = \"desconhecido\"\n",
        "                texto = f\"{nome} ({conf:.2f})\"\n",
        "                # desenhar retângulo/label simples\n",
        "                cv2.putText(frame, texto, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
        "            cv2.imshow(\"Webcam - Reconhecimento Facial\", frame)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "# Para rodar: webcam_demo()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XfL6YsOnufi1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "XfL6YsOnufi1",
        "outputId": "a0eac85e-37d2-478f-e3fd-e96fe0dc0e9f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'modelos/svm_faces.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3905170538.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwebcam_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1889599317.py\u001b[0m in \u001b[0;36mwebcam_demo\u001b[0;34m(threshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwebcam_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"modelos/svm_faces.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"modelos/label_encoder.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             with _validate_fileobject_and_memmap(f, filename, mmap_mode) as (\n\u001b[1;32m    737\u001b[0m                 \u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'modelos/svm_faces.pkl'"
          ]
        }
      ],
      "source": [
        "webcam_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bac566d",
      "metadata": {
        "id": "0bac566d"
      },
      "source": [
        "\n",
        "## 12) Dicas de qualidade de dados\n",
        "\n",
        "- Tenha **muitas amostras por pessoa** (ideal 50 a 200).\n",
        "- Varie iluminação, ângulos, expressões e fundos.\n",
        "- Evite óculos escuros/bonés em todas as fotos; inclua alguns casos, mas misture com fotos \"limpas\".\n",
        "- Se um rosto não for detectado, ajuste o `margin` do MTCNN, melhore a qualidade ou filtre imagens ruins.\n",
        "- Use `threshold` (0.60–0.80) para rejeitar desconhecidos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd47c32",
      "metadata": {
        "id": "3fd47c32"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# **Opção B (Alternativa):** Transfer Learning com Keras/MobileNetV2\n",
        "\n",
        "Treina um **classificador com Softmax** diretamente na imagem recortada 160x160 a partir de um backbone `MobileNetV2`.\n",
        "Funciona bem com dados suficientes; porém, para **datasets pequenos**, a abordagem de embeddings (FaceNet + SVM) tende a ser mais estável.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ffa8226",
      "metadata": {
        "id": "5ffa8226"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Rode somente se quiser testar a alternativa com Keras\n",
        "# from tensorflow.keras import layers, models\n",
        "# from tensorflow.keras.applications import MobileNetV2\n",
        "# from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# import tensorflow as tf\n",
        "# import glob\n",
        "\n",
        "# # Carregar caminhos por classe\n",
        "# def paths_labels_from_folder(pasta):\n",
        "#     X, y = [], []\n",
        "#     for pessoa in sorted(os.listdir(pasta)):\n",
        "#         pdir = os.path.join(pasta, pessoa)\n",
        "#         if not os.path.isdir(pdir):\n",
        "#             continue\n",
        "#         for img in glob.glob(os.path.join(pdir, \"*.*\")):\n",
        "#             if os.path.splitext(img)[1].lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\"}:\n",
        "#                 X.append(img); y.append(pessoa)\n",
        "#     return np.array(X), np.array(y)\n",
        "\n",
        "# X_all, y_all = paths_labels_from_folder(PASTA_PROC)\n",
        "# X_tr, X_te, y_tr, y_te = train_test_split(X_all, y_all, test_size=0.2, random_state=SEED, stratify=y_all)\n",
        "\n",
        "# # tf.data para carregar e pre-processar\n",
        "# def load_img_tf(path):\n",
        "#     img = tf.io.read_file(path)\n",
        "#     img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
        "#     img = tf.image.resize(img, (160,160))\n",
        "#     img = tf.cast(img, tf.float32)\n",
        "#     img = preprocess_input(img)  # MobileNetV2\n",
        "#     return img\n",
        "\n",
        "# def gen_ds(X, y, batch=32, training=False):\n",
        "#     ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "#     if training:\n",
        "#         ds = ds.shuffle(buffer_size=len(X), seed=SEED)\n",
        "#     ds = ds.map(lambda p, t: (load_img_tf(p), t), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "#     ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
        "#     return ds\n",
        "\n",
        "# classes = sorted(set(y_all))\n",
        "# le2 = LabelEncoder().fit(classes)\n",
        "# y_tr_enc = le2.transform(y_tr)\n",
        "# y_te_enc = le2.transform(y_te)\n",
        "\n",
        "# ds_tr = gen_ds(X_tr, y_tr_enc, training=True)\n",
        "# ds_te = gen_ds(X_te, y_te_enc, training=False)\n",
        "\n",
        "# base = MobileNetV2(include_top=False, input_shape=(160,160,3), weights='imagenet')\n",
        "# base.trainable = False  # comece congelado; opcionalmente descongele depois\n",
        "\n",
        "# model = models.Sequential([\n",
        "#     base,\n",
        "#     layers.GlobalAveragePooling2D(),\n",
        "#     layers.Dropout(0.3),\n",
        "#     layers.Dense(len(classes), activation='softmax')\n",
        "# ])\n",
        "\n",
        "# model.compile(optimizer=Adam(1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "# model.summary()\n",
        "\n",
        "# hist = model.fit(ds_tr, validation_data=ds_te, epochs=10)\n",
        "# model.evaluate(ds_te)\n",
        "\n",
        "# # Descongelar parte do backbone (opcional)\n",
        "# # base.trainable = True\n",
        "# # for layer in base.layers[:-40]:\n",
        "# #     layer.trainable = False\n",
        "# # model.compile(optimizer=Adam(1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "# # hist2 = model.fit(ds_tr, validation_data=ds_te, epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70ef3509",
      "metadata": {
        "id": "70ef3509"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### Pronto!\n",
        "- Preencha `dados_brutos/` com as suas fotos e rode as células na ordem.\n",
        "- Se quiser integrar ao seu `\"[ Aula 17 ] - CKP04.ipynb\"`, copie as seções que fizerem mais sentido.\n",
        "- Se travar em algum erro específico, me mande o **stacktrace** que eu ajusto o notebook para você. :)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "31642abc58cf4c0fb9d7ffac2b3412b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_037a7613405f4611a07c789e21ee0906",
              "IPY_MODEL_2a965a95eb66414998c013e73ffbf096",
              "IPY_MODEL_2246ede09d46419882f8394e989b8358"
            ],
            "layout": "IPY_MODEL_c5feb14aa3744c2fb4e8a80de4893632"
          }
        },
        "037a7613405f4611a07c789e21ee0906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67bd5af61761458c8d5588cee012b94c",
            "placeholder": "​",
            "style": "IPY_MODEL_be59e2bb1082484db8b996c223eab63f",
            "value": "100%"
          }
        },
        "2a965a95eb66414998c013e73ffbf096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d8bbf8a14b6497596070b66a1ed7c21",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_781dee9b8fb649aeb784db129d81122a",
            "value": 111898327
          }
        },
        "2246ede09d46419882f8394e989b8358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2a499975c9048e69ffffc445cf27265",
            "placeholder": "​",
            "style": "IPY_MODEL_9805257aa0294f3b93d3994d741cd443",
            "value": " 107M/107M [00:00&lt;00:00, 223MB/s]"
          }
        },
        "c5feb14aa3744c2fb4e8a80de4893632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67bd5af61761458c8d5588cee012b94c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be59e2bb1082484db8b996c223eab63f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d8bbf8a14b6497596070b66a1ed7c21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "781dee9b8fb649aeb784db129d81122a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2a499975c9048e69ffffc445cf27265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9805257aa0294f3b93d3994d741cd443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}